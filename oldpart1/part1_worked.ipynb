{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from scipy import *\n",
    "from numpy import *\n",
    "from scipy.ndimage import *\n",
    "import pylab\n",
    "#!/usr/bin/env python\n",
    "import cv2\n",
    "from scipy import *\n",
    "from scipy.linalg import *\n",
    "from scipy.special import *\n",
    "from random import choice\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python module for use with David Lowe's SIFT code available at:\n",
    "http://www.cs.ubc.ca/~lowe/keypoints/\n",
    "adapted from the matlab code examples.\n",
    "\n",
    "Initial code by Jan Erik Solem, 2009-01-30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def match(desc1,desc2):\n",
    "    \"\"\" \n",
    "        for each descriptor in the first image, select its match to second image    \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    dist_ratio = 0.6\n",
    "    desc1_size = desc1.shape\n",
    "\n",
    "    matchscores = zeros((desc1_size[0],1))\n",
    "    desc2t = desc2.T #precompute matrix transpose\n",
    "    for i in range(desc1_size[0]):\n",
    "        dotprods = dot(desc1[i,:],desc2t) #vector of dot products\n",
    "        dotprods = 0.9999*dotprods\n",
    "        #inverse cosine and sort, return index for features in second image\n",
    "        indx = argsort(arccos(dotprods))\n",
    "\n",
    "        #check if nearest neighbor has angle less than dist_ratio times 2nd\n",
    "        if arccos(dotprods)[indx[0]] < dist_ratio * arccos(dotprods)[indx[1]]:\n",
    "            matchscores[i] = indx[0]\n",
    "\n",
    "    return matchscores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "def appendimages(im1, im2):\n",
    "    \"\"\" Return a new concatenated images side-by-side \"\"\"\n",
    "    if ndim(im1) == 2:\n",
    "        return _appendimages(im1, im2)\n",
    "    else:\n",
    "        imr = _appendimages(im1[:, :, 0], im2[:, :, 0])\n",
    "        img = _appendimages(im1[:, :, 1], im2[:, :, 1])\n",
    "        imb = _appendimages(im1[:, :, 2], im2[:, :, 2])\n",
    "        return dstack((imr, img, imb))\n",
    "\n",
    "\n",
    "def _appendimages(im1,im2):\n",
    "    \"\"\" return a new image that appends the two images side-by-side.\"\"\"\n",
    "\n",
    "    #select the image with the fewest rows and fill in enough empty rows\n",
    "    rows1 = im1.shape[0]\n",
    "    rows2 = im2.shape[0]\n",
    "\n",
    "    if rows1 < rows2:\n",
    "        im1 = concatenate((im1,zeros((rows2-rows1,im1.shape[1]))), axis=0)\n",
    "    else:\n",
    "        im2 = concatenate((im2,zeros((rows1-rows2,im2.shape[1]))), axis=0)\n",
    "\n",
    "    return concatenate((im1,im2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import *\n",
    "from scipy import linalg\n",
    "from scipy import ndimage\n",
    "\n",
    "'''\n",
    "    Homography\n",
    "'''\n",
    "\n",
    "def Haffine_from_points(fp,tp):\n",
    "    \"\"\" find H, affine transformation, such that\n",
    "        tp is affine transf of fp\"\"\"\n",
    "\n",
    "    if fp.shape != tp.shape:\n",
    "        raise RuntimeError\n",
    "\n",
    "    #condition points\n",
    "    #-from points-\n",
    "    m = mean(fp[:2], axis=1)\n",
    "    maxstd = max(std(fp[:2], axis=1))\n",
    "    C1 = diag([1/maxstd, 1/maxstd, 1])\n",
    "    C1[0][2] = -m[0]/maxstd\n",
    "    C1[1][2] = -m[1]/maxstd\n",
    "    fp_cond = dot(C1,fp)\n",
    "\n",
    "    #-to points-\n",
    "    m = mean(tp[:2], axis=1)\n",
    "    C2 = C1.copy() #must use same scaling for both point sets\n",
    "    C2[0][2] = -m[0]/maxstd\n",
    "    C2[1][2] = -m[1]/maxstd\n",
    "    tp_cond = dot(C2,tp)\n",
    "\n",
    "    #conditioned points have mean zero, so translation is zero\n",
    "    A = concatenate((fp_cond[:2],tp_cond[:2]), axis=0)\n",
    "    U,S,V = linalg.svd(A.T)\n",
    "\n",
    "    #create B and C matrices as Hartley-Zisserman (2:nd ed) p 130.\n",
    "    tmp = V[:2].T\n",
    "    B = tmp[:2]\n",
    "    C = tmp[2:4]\n",
    "\n",
    "    tmp2 = concatenate((dot(C,linalg.pinv(B)),zeros((2,1))), axis=1)\n",
    "    H = vstack((tmp2,[0,0,1]))\n",
    "\n",
    "    #decondition\n",
    "    H = dot(linalg.inv(C2),dot(H,C1))\n",
    "\n",
    "    return H / H[2][2]\n",
    "\n",
    "def affine_transform2(im, rot, shift):\n",
    "    '''\n",
    "        Perform affine transform for 2/3D images.\n",
    "    '''\n",
    "    if ndim(im) == 2:\n",
    "        return ndimage.affine_transform(im, rot, shift)\n",
    "    else:\n",
    "        imr = ndimage.affine_transform(im[:, :, 0], rot, shift)\n",
    "        img = ndimage.affine_transform(im[:, :, 1], rot, shift)\n",
    "        imb = ndimage.affine_transform(im[:, :, 2], rot, shift)\n",
    "\n",
    "        return dstack((imr, img, imb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_points(kp1, kp2, matchscores):\n",
    "    '''\n",
    "        Return the corresponding points in both the images\n",
    "    '''\n",
    "    plist = []\n",
    "    t = min(len(locs1), len(locs2))\n",
    "    for i in range(len(matchscores)):\n",
    "        if (matchscores[i] > 0):\n",
    "            y1 = int(locs1[i, 1])\n",
    "            x1 = int(locs1[i, 0])\n",
    "\n",
    "            y2 = int(locs2[int(matchscores[i]), 1])\n",
    "            x2 = int(locs2[int(matchscores[i]), 0])\n",
    "\n",
    "            plist.append([[x1,y1],[x2,y2]])\n",
    "    return plist\n",
    "\n",
    "def get_homography(points_list):\n",
    "    '''\n",
    "        Function to quickly compute a homography matrix from all point \n",
    "        correspondences.\n",
    "\n",
    "        Inputs:\n",
    "            points_list: tuple of tuple of tuple of correspondence indices. Each\n",
    "            entry is [[x1, y1], [x2, y2]] where [x1, y1] from image 1 corresponds\n",
    "            to [x2, y2] from image 2.\n",
    "\n",
    "        Outputs:\n",
    "            H: Homography matrix.\n",
    "    '''\n",
    "    fp = ones((len(points_list), 3))\n",
    "    tp = ones((len(points_list), 3))\n",
    "\n",
    "    for idx in range(len(points_list)):\n",
    "        fp[idx, 0] = points_list[idx][0][0]\n",
    "        fp[idx, 1] = points_list[idx][0][1]\n",
    "\n",
    "        tp[idx, 0] = points_list[idx][1][0]\n",
    "        tp[idx, 1] = points_list[idx][1][1]\n",
    "\n",
    "    H = Haffine_from_points(fp.T, tp.T)\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def ransac(im1, im2, points_list, iters = 300 , error = 10, good_model_num = 15):\n",
    "    '''\n",
    "        This function uses RANSAC algorithm to estimate the\n",
    "        shift and rotation between the two given images\n",
    "    '''\n",
    "\n",
    "    if ndim(im1) == 2:\n",
    "        rows,cols = im1.shape\n",
    "    else:\n",
    "        rows, cols, _ = im1.shape\n",
    "\n",
    "    model_error = 255\n",
    "    model_H = None\n",
    "\n",
    "    for i in range(iters):\n",
    "        consensus_set = []\n",
    "        points_list_temp = copy(points_list).tolist()\n",
    "        # Randomly select 3 points\n",
    "        for j in range(3):\n",
    "            temp = choice(points_list_temp)\n",
    "            consensus_set.append(temp)\n",
    "            points_list_temp.remove(temp)\n",
    "\n",
    "        # Calculate the homography matrix from the 3 points\n",
    "\n",
    "        fp0 = []\n",
    "        fp1 = []\n",
    "        fp2 = []\n",
    "\n",
    "        tp0 = []\n",
    "        tp1 = []\n",
    "        tp2 = []\n",
    "        for line in consensus_set:\n",
    "\n",
    "            fp0.append(line[0][0])\n",
    "            fp1.append(line[0][1])\n",
    "            fp2.append(1)\n",
    "\n",
    "            tp0.append(line[1][0])\n",
    "            tp1.append(line[1][1])\n",
    "            tp2.append(1)\n",
    "\n",
    "        fp = array([fp0, fp1, fp2])\n",
    "        tp = array([tp0, tp1, tp2])\n",
    "\n",
    "        H = Haffine_from_points(fp, tp)\n",
    "\n",
    "        # Transform the second image\n",
    "        # imtemp = transform_im(im2, [-xshift, -yshift], -theta)\n",
    "        # Check if the other points fit this model\n",
    "\n",
    "        for p in points_list_temp:\n",
    "            x1, y1 = p[0]\n",
    "            x2, y2 = p[1]\n",
    "\n",
    "            A = array([x1, y1, 1]).reshape(3,1)\n",
    "            B = array([x2, y2, 1]).reshape(3,1)\n",
    "\n",
    "            out = B - dot(H, A)\n",
    "            dist_err = hypot(out[0][0], out[1][0])\n",
    "            if dist_err < error:\n",
    "                consensus_set.append(p)\n",
    "\n",
    "\n",
    "        # print(len(consensus_set))\n",
    "        # Check how well is our speculated model\n",
    "        if len(consensus_set) >= good_model_num:\n",
    "            dists = []\n",
    "            for p in consensus_set:\n",
    "                x0, y0 = p[0]\n",
    "                x1, y1 = p[1]\n",
    "\n",
    "                A = array([x0, y0, 1]).reshape(3,1)\n",
    "                B = array([x1, y1, 1]).reshape(3,1)\n",
    "\n",
    "                out = B - dot(H, A)\n",
    "                dist_err = hypot(out[0][0], out[1][0])\n",
    "                dists.append(dist_err)\n",
    "            if (max(dists) < error) and (max(dists) < model_error):\n",
    "                model_error = max(dists)\n",
    "                model_H = H\n",
    "    \n",
    "    print(model_H)\n",
    "    return model_H\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_new(data, modelClass, minDataPts, nIter, threshold, nCloseRequired):\n",
    "    best_model = None\n",
    "    best_consensus_set = None\n",
    "    best_error = np.inf\n",
    "    for iteration in xrange(nIter):\n",
    "        maybe_inliers = data[random.sample(range(len(data)),minDataPts)]\n",
    "        maybe_model = modelClass()\n",
    "        maybe_model.fit(maybe_inliers)\n",
    "\n",
    "        fit_errors = maybe_model.calc_error(data)\n",
    "        consensus_set = np.flatnonzero(fit_errors < threshold)\n",
    "        \n",
    "        if len(consensus_set) > nCloseRequired:\n",
    "            maybe_model.fit(data[consensus_set])\n",
    "            new_fit_errors = maybe_model.calc_error(data)\n",
    "            total_error = new_fit_errors.sum()\n",
    "            if total_error < best_error:\n",
    "                best_model = maybe_model\n",
    "                best_consensus_set = np.flatnonzero(new_fit_errors < threshold) # here I differ from wikipedia\n",
    "                best_error = total_error\n",
    "\n",
    "    return best_model, best_consensus_set, best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_images(image0, image1, transform):\n",
    "    r, c = image1.shape[:2]\n",
    "    # Note that transformations take coordinates in (x, y) format,\n",
    "    # not (row, column), in order to be consistent with most literature\n",
    "    corners = np.array([[0, 0],\n",
    "                        [0, r],\n",
    "                        [c, 0],\n",
    "                        [c, r]])\n",
    "\n",
    "    # Warp the image corners to their new positions\n",
    "    warped_corners = transform(corners)\n",
    "\n",
    "    # Find the extents of both the reference image and the warped\n",
    "    # target image\n",
    "    all_corners = np.vstack((warped_corners, corners))\n",
    "\n",
    "    corner_min = np.min(all_corners, axis=0)\n",
    "    corner_max = np.max(all_corners, axis=0)\n",
    "\n",
    "    output_shape = (corner_max - corner_min)\n",
    "    output_shape = np.ceil(output_shape[::-1])\n",
    "\n",
    "    offset = SimilarityTransform(translation=-corner_min)\n",
    "\n",
    "    image0_ = warp(image0, offset.inverse, output_shape=output_shape, cval=-1)\n",
    "\n",
    "    image1_ = warp(image1, (transform + offset).inverse, output_shape=output_shape, cval=-1)\n",
    "\n",
    "    image0_zeros = warp(image0, offset.inverse, output_shape=output_shape, cval=0)\n",
    "\n",
    "    image1_zeros = warp(image1, (transform + offset).inverse, output_shape=output_shape, cval=0)\n",
    "\n",
    "    overlap = (image0_ != -1.0 ).astype(int) + (image1_ != -1.0).astype(int)\n",
    "    overlap += (overlap < 1).astype(int)\n",
    "    merged = (image0_zeros+image1_zeros)/overlap\n",
    "\n",
    "    im = Image.fromarray((255*merged).astype('uint8'), mode='RGB')\n",
    "    im.save('stitched_images.jpg')\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_left_path = \"data\\\\part1\\\\left.jpg\"\n",
    "img_1_right_path = \"data\\\\part1\\\\right.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def plot_matches(img1,img2,locs):\n",
    "    \"\"\" show image with features. input: im (image as array),\n",
    "        locs (row, col, scale, orientation of each feature) \"\"\"\n",
    "    \n",
    "    implot = plt.imshow(img1)    \n",
    "    for points in locs:\n",
    "        x,y = points[0]\n",
    "        plt.scatter(x,y)\n",
    "    plt.show()\n",
    "    \n",
    "    implot2 = plt.imshow(img2)\n",
    "    for points in locs:\n",
    "        x,y = points[1]\n",
    "        plt.scatter(x,y)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "img1_l = Image.open(img_1_left_path)\n",
    "img1_r = Image.open(img_1_right_path)\n",
    "\n",
    "im1_l = asarray(img1_l)\n",
    "im1_r = asarray(img1_r)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "'''\n",
    "    kp is a list of key points\n",
    "    des is a numpy array of shape Number_of_Keypoints×128.\n",
    "'''\n",
    "kp1, des1 = sift.detectAndCompute(im1_l, None)\n",
    "kp2, des2 = sift.detectAndCompute(im1_r, None)\n",
    "# print(\"desc shape\", des1)\n",
    "distance = cdist(des1, des2, 'sqeuclidean')\n",
    "\"\"\"\n",
    "Select putative matches based on the matrix of pairwise descriptor distances obtained above. \n",
    "You can select all pairs whose descriptor distances are below a specified threshold, \n",
    "or select the top few hundred descriptor pairs with the smallest pairwise distances.\n",
    "\"\"\"\n",
    "# num of points to extract \n",
    "total_select_numbers = min(300, len(kp1), len(kp2))\n",
    "select_match = []\n",
    "INF = 1111111\n",
    "\n",
    "for i in range(total_select_numbers):\n",
    "        ri,ci = np.unravel_index(distance.argmin(), distance.shape)\n",
    "        x1,y1 = kp1[ri].pt\n",
    "        x1,y1 = int(x1),int(y1)\n",
    "        x2,y2 = kp2[ci].pt\n",
    "        x2,y2 = int(x2),int(y2)\n",
    "        select_match.append([[x1,y1],[x2,y2]])\n",
    "        # set to INF after used\n",
    "        distance[ri, :] = INF\n",
    "        distance[:, ci] = INF\n",
    "\n",
    "print(len(select_match))\n",
    "plot_matches(img1_l,img1_r,select_match)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.90531716e-01  3.44193097e+00 -7.05756996e+02]\n [ 1.15671642e-01  6.30597015e-01  2.54253731e+01]\n [ 0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # Compare ransac and simple homography matrix\n",
    " \n",
    " #workable param: iterationransac(im1, im2, points_list, \n",
    " # iters = 300 , error = 10, good_model_num = 15):\n",
    "\n",
    "out_ransac = ransac(im1_l, im1_r, select_match,\n",
    "                    iters = 300 ,\n",
    "                    error=10,\n",
    "                    good_model_num=10\n",
    "                    )\n",
    "out_simple = get_homography(select_match)\n",
    "\n",
    "H_ransac = inv(out_ransac)\n",
    "H_simple = inv(out_simple)\n",
    "\n",
    "im_ransac = affine_transform2(im1_l,\n",
    "                              H_ransac[:2, :2],\n",
    "                              [H_ransac[0][2], H_ransac[1][2]])\n",
    "\n",
    "im_simple = affine_transform2(im1_l,\n",
    "                              H_simple[:2, :2],\n",
    "                              [H_simple[0][2], H_simple[1][2]])\n",
    "Image.fromarray(im1_r).show()\n",
    "Image.fromarray(im_ransac).show()\n",
    "Image.fromarray(im_simple).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Stitcher' object has no attribute 'detectAndDescribe'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-9ff24b8c7976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstitcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStitcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstitcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg1_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg1_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-9a62ee004406>\u001b[0m in \u001b[0;36mstitch\u001b[0;34m(self, images, ratio, reprojThresh)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcachedH\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# detect keypoints and extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mkpsA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndDescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mkpsB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndDescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Stitcher' object has no attribute 'detectAndDescribe'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
